name: Train and Deploy Model

on:
  push:
    branches: [ develop ]

jobs:
  train-and-deploy:
    runs-on: ubuntu-latest
    
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_REGION }}
      DATETIME: $(date +'%Y%m%d_%H%M%S')
      PYTHONPATH: /home/runner/work/E2E-Model-Deployment/E2E-Model-Deployment/model-training

    steps:
    - uses: actions/checkout@v2
      with:
        fetch-depth: 0 

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r model-training/requirements.txt
        pip install dvc dvc[s3] boto3 markdown

    - name: Configure DVC
      run: |
        # Configure remote with endpoint URL
        dvc remote add -d s3remote s3://${{ secrets.DVC_BUCKET }} -f

    - name: Fetch Data
      run: |
        cd model-training
        python src/data/fetch_data.py

    - name: Train model
      run: |
        cd model-training
        export PYTHONPATH=$PYTHONPATH:$(pwd)
        python src/train.py dataset=combined

    - name: Run Inference on Valid Set
      run: |
        cd model-training
        # Debug: Print current directory and PYTHONPATH
        pwd
        echo $PYTHONPATH
        # Create output directory
        mkdir -p infer_samples/output
        # Run inference with Hydra override
        python src/infer.py dataset=combined validation_only=true

    - name: Prepare artifacts for S3
      run: |
        COMMIT_ID=$(git rev-parse --short HEAD)
        DATETIME=$(date +'%Y%m%d_%H%M%S')
        
        # Create directory structure
        mkdir -p "${DATETIME}/${COMMIT_ID}/model-data/results"
        mkdir -p "${DATETIME}/${COMMIT_ID}/model-data/model-weights"
        mkdir -p "${DATETIME}/${COMMIT_ID}/model-data/inference"
        
        # Copy results, model weights, and inference results
        cp model-training/model-data/results/*.json "${DATETIME}/${COMMIT_ID}/model-data/results/"
        cp model-training/model-data/models/*.ckpt "${DATETIME}/${COMMIT_ID}/model-data/model-weights/"
        cp -r model-training/infer_samples/output/* "${DATETIME}/${COMMIT_ID}/model-data/inference/"

    - name: Upload to S3
      run: |
        COMMIT_ID=$(git rev-parse --short HEAD)
        DATETIME=$(date +'%Y%m%d_%H%M%S')
        
        aws s3 sync "${DATETIME}/${COMMIT_ID}/model-data/" s3://${{ secrets.MODEL_BUCKET }}/${DATETIME}/${COMMIT_ID}/model-data/

    - uses: iterative/setup-cml@v2

    - name: Generate Performance Report
      id: report
      env:
        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        COMMIT_ID=$(git rev-parse --short HEAD)
        DATETIME=$(date +'%Y%m%d_%H%M%S')
        
        # Read metrics from JSON
        METRICS=$(cat model-training/model-data/results/combined_resnet18_results.json)
        
        # Count inference samples
        INFER_COUNT=$(ls -1 model-training/infer_samples/output | wc -l)
        
        # Create report
        cat << EOF > report.md
        ### Model Training Report
        **Commit:** ${COMMIT_ID}
        **Timestamp:** ${DATETIME}
        
        #### Performance Metrics
        \`\`\`json
        $METRICS
        \`\`\`
        
        #### Validation Set Inference
        - Number of samples processed: ${INFER_COUNT}
        - Inference results stored in S3
        
        #### Storage Location
        s3://${{ secrets.MODEL_BUCKET }}/${DATETIME}/${COMMIT_ID}/model-data/
        EOF
        
        # Create CML report
        cml comment create report.md 