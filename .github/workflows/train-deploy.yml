name: Train and Deploy Model

on:
  push:
    branches: [ develop ]

# Add permissions block at the top level
permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  train-and-deploy:
    runs-on: ubuntu-latest
    
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_REGION }}
      PYTHONPATH: /home/runner/work/E2E-Model-Deployment/E2E-Model-Deployment/model-training

    steps:
    - uses: actions/checkout@v2
      with:
        fetch-depth: 0 

    - name: Set Environment Variables
      run: |
        # Set variables
        DATETIME=$(date +'%Y%m%d_%H%M%S')
        COMMIT_ID=$(git rev-parse --short HEAD)
        
        # Set for subsequent steps
        echo "DATETIME=$DATETIME" >> $GITHUB_ENV
        echo "COMMIT_ID=$COMMIT_ID" >> $GITHUB_ENV
        
        # Debug: Print using shell variables
        echo "Setting DATETIME: $DATETIME"
        echo "Setting COMMIT_ID: $COMMIT_ID"
        echo "PYTHONPATH: ${PYTHONPATH}"

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r model-training/requirements.txt
        pip install dvc dvc[s3] boto3 markdown

    - name: Configure DVC
      run: |
        # Configure remote with endpoint URL
        dvc remote add -d s3remote s3://${{ secrets.DVC_BUCKET }} -f

    - name: Fetch Data
      run: |
        cd model-training
        python src/data/fetch_data.py

    - name: Train model
      run: |
        cd model-training
        export PYTHONPATH=$PYTHONPATH:$(pwd)
        python src/train.py dataset=combined

    - name: Run Inference on Valid Set
      run: |
        cd model-training
        # Debug: Print current directory and PYTHONPATH
        pwd
        echo $PYTHONPATH
        # Create output directory
        mkdir -p infer_samples/output
        # Run inference with Hydra override
        python src/infer.py dataset=combined validation_only=true

    - name: Prepare artifacts for S3
      run: |
        echo "Creating directories for artifacts..."
        mkdir -p artifacts
        
        # Create full directory structure using environment variables
        mkdir -p "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/results"
        mkdir -p "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/model-weights"
        mkdir -p "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/inference"
        
        echo "Copying artifacts..."
        # Copy results, model weights, and inference results
        cp -v model-training/model-data/results/*.json "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/results/"
        cp -v model-training/model-data/models/*.pt "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/model-weights/"
        cp -rv model-training/infer_samples/output/* "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/inference/"

    - name: Upload to S3
      run: |
        echo "Uploading to S3..."
        aws s3 sync "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/" s3://${{ secrets.MODEL_BUCKET }}/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/

    - uses: iterative/setup-cml@v2

    - name: Generate Performance Report
      id: report
      env:
        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Read metrics from JSON
        METRICS=$(cat model-training/model-data/results/combined_resnet18_results.json)
        
        # Count inference samples
        INFER_COUNT=$(ls -1 model-training/infer_samples/output | wc -l)
        
        # Create report
        cat << EOF > report.md
        ### Model Training Report
        **Commit:** ${{ env.COMMIT_ID }}
        **Timestamp:** ${{ env.DATETIME }}
        
        #### Performance Metrics
        \`\`\`json
        $METRICS
        \`\`\`
        
        #### Validation Set Inference
        - Number of samples processed: ${INFER_COUNT}
        - Inference results stored in S3
        
        #### Storage Location
        s3://${{ secrets.MODEL_BUCKET }}/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/
        EOF
        
        # Create CML report
        cml-publish report.md --md > report.md
        echo "::set-output name=report::$(cat report.md)" 