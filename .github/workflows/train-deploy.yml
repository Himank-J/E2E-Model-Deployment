name: Train and Deploy Model

on:
  push:
    branches: [ develop ]

jobs:
  train-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      issues: write
      id-token: write
    
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_REGION }}
      PYTHONPATH: /home/runner/work/E2E-Model-Deployment/E2E-Model-Deployment/model-training

    steps:
    - uses: actions/checkout@v2
      with:
        fetch-depth: 0 

    - name: Set Environment Variables
      run: |
        # Set variables
        DATETIME=$(date +'%Y%m%d_%H%M%S')
        COMMIT_ID=$(git rev-parse --short HEAD)
        
        # Set for subsequent steps
        echo "DATETIME=$DATETIME" >> $GITHUB_ENV
        echo "COMMIT_ID=$COMMIT_ID" >> $GITHUB_ENV
        
        # Debug: Print using shell variables
        echo "Setting DATETIME: $DATETIME"
        echo "Setting COMMIT_ID: $COMMIT_ID"
        echo "PYTHONPATH: ${PYTHONPATH}"

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r model-training/requirements.txt
        pip install dvc dvc[s3] boto3 markdown

    - name: Configure DVC
      run: |
        # Configure remote with endpoint URL
        dvc remote add -d s3remote s3://${{ secrets.DVC_BUCKET }} -f

    - name: Fetch Data
      run: |
        cd model-training
        python src/data/fetch_data.py

    - name: Train model
      run: |
        cd model-training
        export PYTHONPATH=$PYTHONPATH:$(pwd)
        python src/train.py dataset=combined

    - name: Run Inference on Valid Set
      run: |
        cd model-training
        # Debug: Print current directory and PYTHONPATH
        pwd
        echo $PYTHONPATH
        # Create output directory
        mkdir -p infer_samples/output
        # Run inference with Hydra override
        python src/infer.py dataset=combined validation_only=true

    - name: Prepare artifacts for S3
      run: |
        echo "Creating directories for artifacts..."
        mkdir -p artifacts
        
        # Create full directory structure using environment variables
        mkdir -p "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/results"
        mkdir -p "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/model-weights"
        mkdir -p "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/inference"
        
        echo "Copying artifacts..."
        # Copy results, model weights, and inference results
        cp -v model-training/model-data/results/*.json "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/results/"
        cp -v model-training/model-data/models/*.pt "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/model-weights/"
        cp -rv model-training/infer_samples/output/* "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/inference/"

    - name: Upload to S3
      run: |
        echo "Uploading to S3..."
        aws s3 sync "artifacts/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/" s3://${{ secrets.MODEL_BUCKET }}/${{ env.DATETIME }}/${{ env.COMMIT_ID }}/model-data/

    - uses: iterative/setup-cml@v2

    - name: Generate Performance Report
      run: |
        python model-training/src/utils/generate_baseline_report.py \
          model-training/model-data/results/combined_resnet18_results.json \
          model-training/infer_samples/output \
          ${{ env.COMMIT_ID }} \
          ${{ env.DATETIME }} \
          ${{ secrets.MODEL_BUCKET }}

    - name: Comment on Commit
      env:
        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Create CML report from the generated markdown
        cml comment create baseline_report.md 